{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Screenshot Categorization\n",
    "\n",
    "This notebook has two datasets with different jobs:\n",
    "\n",
    "1. Labeled slice from `samples.py` + manual labels built via `www/` tool. A supervised evaluation.\n",
    "2. Full screenshot corpus is resolved from config paths. OCR/CLIP caching, clustering, and co-occurence of manual labels.\n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "- **Label**: manually assigned category from the `www/` labeling tool.\n",
    "- **Feature**: numerical signal used by a model: TF-IDF weights, CLIP embedding dimensions.\n",
    "- **Embedding**: a dense vector representation of an image. CLIP uses 768 dims.\n",
    "- **Prediction**: model output for one or more labels.\n",
    "\n",
    "## Goals\n",
    "\n",
    "- Test whether OCR text features can carry the screenshot label space.\n",
    "- Compare that to CLIP visual embeddings on the same labeled slice.\n",
    "- If CLIP wins clearly, use CLIP clustering on the full corpus to speed manual labeling in `www/` (cluster slice, multi-select, remove outliers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def stopwatch(flag):\n",
    "    if flag:\n",
    "        stopwatch.s = time.perf_counter()\n",
    "    else:\n",
    "        print(time.perf_counter() - stopwatch.s)\n",
    "\n",
    "\n",
    "stopwatch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPModel, CLIPProcessor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Set shared data paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root(marker=\"data/labels.jsonl\"):\n",
    "    for p in (Path.cwd(), *Path.cwd().parents):\n",
    "        if (p / marker).exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(marker)\n",
    "\n",
    "\n",
    "ROOT = find_root()\n",
    "LABELS_PATH = ROOT / \"data/labels.jsonl\"\n",
    "\n",
    "rows = [json.loads(line) for line in LABELS_PATH.read_text().splitlines() if line.strip()]\n",
    "paths = [ROOT / r[\"input_path\"] for r in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "### Helpers\n",
    "- `make_df` wraps classifier output into a labeled DataFrame indexed by filename.\n",
    "- `top_k` slices that DataFrame by label column\n",
    "- and returns ranked hits as a list\n",
    "of `{path, score}` dicts  the currency everything else operates on.\n",
    "- `top_labels` picks the *n* highest-mass columns by summing probability across\n",
    "all images. This is useful for a first-pass survey of what the model is confident about.\n",
    "- `print_hits` and `plot_hits` produce table and gallery of images and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "THRESHOLD = 0.2\n",
    "\n",
    "\n",
    "def make_df(probs, binarizer):\n",
    "    return pd.DataFrame(probs, columns=binarizer.classes_, index=[p.name for p in paths])\n",
    "\n",
    "\n",
    "def top_k(df, label, k=12):\n",
    "    scores = df[label].to_numpy()\n",
    "    order = np.argsort(scores)[::-1][:k]\n",
    "    return [{\"path\": paths[i], \"score\": scores[i]} for i in order]\n",
    "\n",
    "\n",
    "def top_labels(df, n=5):\n",
    "    return df.sum().nlargest(n).index.tolist()\n",
    "\n",
    "\n",
    "def print_hits(hits):\n",
    "    for h in hits:\n",
    "        print(f\"{h['score']:.4f}  {h['path'].parent.name}/{h['path'].name}\")\n",
    "\n",
    "\n",
    "def plot_hits(hits, columns=4, label=\"\"):\n",
    "    rows_ = (len(hits) + columns - 1) // columns\n",
    "    fig, axes = plt.subplots(rows_, columns, figsize=(3 * columns, 3 * rows_))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    for ax, hit in zip(axes, hits):\n",
    "        ax.imshow(Image.open(hit[\"path\"]))\n",
    "        ax.set_title(f\"{hit['score']:.3f}\\n{hit['path'].name}\")\n",
    "        ax.axis(\"off\")\n",
    "    for ax in axes[len(hits) :]:\n",
    "        ax.axis(\"off\")\n",
    "    if label:\n",
    "        fig.suptitle(label)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def split_indices(n):\n",
    "    idx = np.random.RandomState(SEED).permutation(n)\n",
    "    cut = int(n * (1 - TEST_SIZE))\n",
    "    return idx[:cut], idx[cut:]\n",
    "\n",
    "\n",
    "def ensure_label_coverage(targets, train_idx, test_idx):\n",
    "    train_set, test_set = set(train_idx.tolist()), set(test_idx.tolist())\n",
    "    for col in range(targets.shape[1]):\n",
    "        if targets[list(train_set), col].sum() > 0:\n",
    "            continue\n",
    "        candidates = [i for i in test_set if targets[i, col] == 1]\n",
    "        if candidates:\n",
    "            test_set.remove(candidates[0])\n",
    "            train_set.add(candidates[0])\n",
    "    return np.array(sorted(train_set)), np.array(sorted(test_set))\n",
    "\n",
    "\n",
    "def numeric_density(text):\n",
    "    return sum(c.isdigit() for c in text) / max(len(text), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = Counter(label for r in rows for label in r[\"categories\"])\n",
    "s = pd.Series(label_counts).sort_values()\n",
    "print(f\"total labels: {len(s)}\")\n",
    "print(f\"labels with <5  examples: {(s < 5).sum()}\")\n",
    "print(f\"labels with <10 examples: {(s < 10).sum()}\")\n",
    "print(f\"labels with >=5 examples: {(s >= 5).sum()}\")\n",
    "print(f\"labels with >=10 examples: {(s >= 10).sum()}\")\n",
    "\n",
    "viable = s[s >= 5].index.tolist()\n",
    "print(f\"\\nviable labels: {viable}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Text Model: OCR Baseline\n",
    "\n",
    "OCR is evaluated as a **baseline** because screenshots often include text. Does OCR text provide enough signal to classify screenshot intent reliably?\n",
    "\n",
    "Expected strengths:\n",
    "\n",
    "1. Lexical cues (`receipt`, `terminal`, site names, error strings)\n",
    "2. Numeric patterns (prices, sports scores, timestamps)\n",
    "3. Repeated UI text fragments (My H2 Title) or graphics (`gd` = green dot status indicator that tesseract picks up)\n",
    "\n",
    "Weakness:\n",
    "\n",
    "- Many labels are visual/structural and not lexical. OCR noise can erase useful distinctions.\n",
    "\n",
    "This section is very much a viability test. If text macro-F1 stays low on the viable label set, OCR is treated as auxiliary metadata rather than the primary representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Run OCR\n",
    "\n",
    "this part will take some time execute, but only once since this method has caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.insert(0, str(ROOT / \"ml\"))\n",
    "import pytesseract\n",
    "from handler import load_config, resolve_image_paths\n",
    "\n",
    "config = load_config(ROOT / \"config.yaml\")\n",
    "resolved = resolve_image_paths(config, series_prefix=\"screenshot\")\n",
    "all_image_paths = [r.path for r in resolved]\n",
    "\n",
    "print(f\"resolved {len(all_image_paths)} images\")\n",
    "\n",
    "\n",
    "def ocr_image(path: Path, psm: int = 6, oem: int = 3) -> str:\n",
    "    with Image.open(path) as img:\n",
    "        if img.size[0] < 10 or img.size[1] < 10:\n",
    "            return \"\"\n",
    "        return pytesseract.image_to_string(\n",
    "            img.convert(\"RGB\"), lang=\"eng\", config=f\"--psm {psm} --oem {oem}\"\n",
    "        )\n",
    "\n",
    "\n",
    "def ocr_output_path(image_path: Path) -> Path:\n",
    "    out = ROOT / \"data/ocr\" / image_path.parent.name / (image_path.stem + \".txt\")\n",
    "    out.parent.mkdir(parents=True, exist_ok=True)\n",
    "    return out\n",
    "\n",
    "\n",
    "def run_ocr_incremental(image_paths: list[Path]) -> dict[Path, Path]:\n",
    "    mapping = {}\n",
    "    for path in tqdm(image_paths, desc=\"ocr\"):\n",
    "        out = ocr_output_path(path)\n",
    "        if not out.exists():\n",
    "            try:\n",
    "                text = ocr_image(path)\n",
    "            except (OSError, Exception) as e:\n",
    "                print(f\"\\nskipped {path.name}: {e}\")\n",
    "                text = \"\"\n",
    "            out.write_text(text, encoding=\"utf-8\")\n",
    "        mapping[path] = out\n",
    "    return mapping\n",
    "\n",
    "\n",
    "ocr_map = run_ocr_incremental(all_image_paths)\n",
    "print(f\"ocr complete: {len(ocr_map)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Load OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_path(r):\n",
    "    img = Path(r[\"input_path\"])\n",
    "    return ROOT / \"data/ocr\" / img.parent.name / (img.stem + \".txt\")\n",
    "\n",
    "\n",
    "text_ocr = [ocr_path(r).read_text().lower() for r in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "### Corpus: per-label OCR character profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in viable:\n",
    "    indices = [i for i, r in enumerate(rows) if label in r[\"categories\"]]\n",
    "    subset = [text_ocr[i] for i in indices]\n",
    "    lengths = [len(t.split()) for t in subset]\n",
    "    densities = [numeric_density(t) for t in subset]\n",
    "    empty = sum(1 for t in subset if len(t.strip()) < 20)\n",
    "    print(\n",
    "        f\"{label:20} n={len(indices):3}  mean_words={np.mean(lengths):6.0f}  \"\n",
    "        f\"mean_density={np.mean(densities):.3f}  empty={empty}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Top word tokens per viable label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(analyzer=\"word\", min_df=1)\n",
    "X_cv = cv.fit_transform(text_ocr)\n",
    "vocab = np.array(cv.get_feature_names_out())\n",
    "\n",
    "for label in viable:\n",
    "    indices = [i for i, r in enumerate(rows) if label in r[\"categories\"]]\n",
    "    sums = np.asarray(X_cv[indices].sum(axis=0)).squeeze()\n",
    "    top = np.argsort(sums)[::-1][:15]\n",
    "    print(f\"\\n--- {label} ---\")\n",
    "    print(\", \".join(f\"{vocab[i]}({sums[i]:.0f})\" for i in top))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Label Separability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5), max_df=0.95)\n",
    "word_vec = TfidfVectorizer(analyzer=\"word\", min_df=1, max_df=0.95)\n",
    "char_feat = char_vec.fit_transform(text_ocr)\n",
    "word_feat = word_vec.fit_transform(text_ocr)\n",
    "\n",
    "stats_idx = [i for i, r in enumerate(rows) if \"statistics\" in r[\"categories\"]]\n",
    "monitor_idx = [\n",
    "    i\n",
    "    for i, r in enumerate(rows)\n",
    "    if \"monitorat\" in r[\"categories\"] and \"statistics\" not in r[\"categories\"]\n",
    "]\n",
    "\n",
    "for name, feat in [(\"char\", char_feat), (\"word\", word_feat)]:\n",
    "    sim = cosine_similarity(feat[stats_idx], feat[monitor_idx])\n",
    "    print(f\"{name}  statistics vs monitorat: mean={sim.mean():.3f} max={sim.max():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "#### vectorizer + classifier sweep\n",
    "\n",
    "the purpose here is to determine the best model for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "viable_set = set(viable)\n",
    "viable_mask = np.array([any(label in viable_set for label in r[\"categories\"]) for r in rows])\n",
    "viable_idx = np.where(viable_mask)[0]\n",
    "\n",
    "sub_rows = [rows[i] for i in viable_idx]\n",
    "sub_ocr = [text_ocr[i] for i in viable_idx]\n",
    "sub_binarizer = MultiLabelBinarizer(classes=viable)\n",
    "sub_targets = sub_binarizer.fit_transform([r[\"categories\"] for r in sub_rows])\n",
    "sub_density = np.array([numeric_density(t) for t in sub_ocr]).reshape(-1, 1)\n",
    "\n",
    "tr, te = ensure_label_coverage(sub_targets, *split_indices(len(sub_rows)))\n",
    "\n",
    "vectorizers = {\n",
    "    \"char(3,5)\": TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5), max_df=0.95),\n",
    "    \"char(2,4)\": TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 4), max_df=0.95),\n",
    "    \"word\": TfidfVectorizer(analyzer=\"word\", min_df=1, max_df=0.95),\n",
    "    \"word+bi\": TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 2), min_df=1, max_df=0.95),\n",
    "}\n",
    "\n",
    "classifiers = {\n",
    "    \"logreg\": OneVsRestClassifier(LogisticRegression(max_iter=1000, solver=\"liblinear\")),\n",
    "    \"svc\": OneVsRestClassifier(LinearSVC(max_iter=2000)),\n",
    "}\n",
    "\n",
    "for vec_name, vec in vectorizers.items():\n",
    "    feat = vec.fit_transform(sub_ocr)\n",
    "    feat_den = np.hstack([feat.toarray(), sub_density])\n",
    "    for clf_name, clf in classifiers.items():\n",
    "        for feat_label, f in [(vec_name, feat), (f\"{vec_name}+density\", feat_den)]:\n",
    "            clf.fit(f[tr], sub_targets[tr])\n",
    "            raw = (\n",
    "                clf.predict_proba(f[te])\n",
    "                if hasattr(clf, \"predict_proba\")\n",
    "                else (clf.decision_function(f[te]) >= 0).astype(int)\n",
    "            )\n",
    "            preds = (raw >= THRESHOLD).astype(int) if hasattr(clf, \"predict_proba\") else raw\n",
    "            micro = f1_score(sub_targets[te], preds, average=\"micro\", zero_division=0)\n",
    "            macro = f1_score(sub_targets[te], preds, average=\"macro\", zero_division=0)\n",
    "            print(f\"{clf_name:8} {feat_label:30} micro={micro:.3f} macro={macro:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "#### Score \n",
    "\n",
    "Scores and saves the winning vec/clf model after the sweep winner is chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_binarizer = MultiLabelBinarizer()\n",
    "text_targets = text_binarizer.fit_transform([r[\"categories\"] for r in rows])\n",
    "\n",
    "text_vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 4), max_df=0.95)\n",
    "text_features = text_vec.fit_transform(text_ocr)\n",
    "\n",
    "tr, te = ensure_label_coverage(text_targets, *split_indices(len(rows)))\n",
    "text_clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "text_clf.fit(text_features[tr], text_targets[tr])\n",
    "\n",
    "text_probs = text_clf.predict_proba(text_features)\n",
    "text_df = make_df(text_probs, text_binarizer)\n",
    "\n",
    "joblib.dump(\n",
    "    {\"vectorizer\": text_vec, \"classifier\": text_clf, \"binarizer\": text_binarizer},\n",
    "    ROOT / \"data/models/text.joblib\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_df[top_labels(text_df)]\n",
    "text_df[viable]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in text_df[top_labels(text_df)]:\n",
    "    plot_hits(top_k(text_df, label, k=8), label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "## Image model\n",
    "\n",
    "### ResNet18 as a visual feature extractor\n",
    "\n",
    "ResNet was pretrained on natural photographs (ImageNet) and produces embeddings that conflate all screenshots as a generic class--cosine similarity between visually distinct labels was 0.56–0.70, indicating poor separation.\n",
    "\n",
    "Therefore, this model has been removed from the notebook.\n",
    "\n",
    "### CLIP as the image processor\n",
    "\n",
    "CLIP,  `openai/clip-vit-base-patch32`, was pretrained contrastively on web-scraped image-text pairs, which includes UI, screenshots, and documents alongside natural images. The same label pairs that ResNet scored 0.56–0.70 similarity score 0.24–0.45 under CLIP--a meaningful improvement in discriminability across the board.\n",
    "\n",
    "`pooler_output` from the vision encoder is used as the 768-dim embedding.\n",
    "\n",
    "- fixed resolution\n",
    "- no variable patching\n",
    "- CPU-friendly at this corpus size (I only have Intel silicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_processor = CLIPProcessor.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    "    use_fast=False,\n",
    "    local_files_only=True,\n",
    ")\n",
    "clip_model = CLIPModel.from_pretrained(\n",
    "    \"openai/clip-vit-base-patch32\",\n",
    "    local_files_only=True,\n",
    ").vision_model\n",
    "clip_model.eval()\n",
    "\n",
    "\n",
    "def embed_images_clip(image_paths, min_size=10):\n",
    "    vectors = []\n",
    "    skipped = []\n",
    "    for path in tqdm(image_paths, desc=\"embedding\"):\n",
    "        with Image.open(path) as img:\n",
    "            if img.size[0] < min_size or img.size[1] < min_size:\n",
    "                skipped.append(path)\n",
    "                vectors.append(np.zeros(768))\n",
    "                continue\n",
    "            inputs = clip_processor(images=img.convert(\"RGB\"), return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = clip_model(**inputs)\n",
    "            vec = outputs.pooler_output.squeeze(0).cpu().numpy()\n",
    "        vectors.append(vec)\n",
    "    if skipped:\n",
    "        print(f\"skipped {len(skipped)} degenerate images:\")\n",
    "        for p in skipped:\n",
    "            print(f\"  {p.parent.name}/{p.name}\")\n",
    "    return np.vstack(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_binarizer = MultiLabelBinarizer()\n",
    "image_targets = image_binarizer.fit_transform([r[\"categories\"] for r in rows])\n",
    "clip_embeddings = embed_images_clip(paths)\n",
    "\n",
    "print(\n",
    "    f\"samples: {len(rows)}  labels: {len(image_binarizer.classes_)}  \"\n",
    "    f\"dims: {clip_embeddings.shape[1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, te = ensure_label_coverage(image_targets, *split_indices(len(rows)))\n",
    "\n",
    "image_clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "image_clf.fit(clip_embeddings[tr], image_targets[tr])\n",
    "\n",
    "image_probs_test = image_clf.predict_proba(clip_embeddings[te])\n",
    "image_preds_test = (image_probs_test >= THRESHOLD).astype(int)\n",
    "\n",
    "print(\"micro:\", f1_score(image_targets[te], image_preds_test, average=\"micro\", zero_division=0))\n",
    "print(\"macro:\", f1_score(image_targets[te], image_preds_test, average=\"macro\", zero_division=0))\n",
    "print(\n",
    "    classification_report(\n",
    "        image_targets[te], image_preds_test, target_names=image_binarizer.classes_, zero_division=0\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Score and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_probs = image_clf.predict_proba(clip_embeddings)\n",
    "image_df = make_df(image_probs, image_binarizer)\n",
    "\n",
    "joblib.dump(\n",
    "    {\"classifier\": image_clf, \"binarizer\": image_binarizer}, ROOT / \"data/models/image.joblib\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df[top_labels(image_df)].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for label in [\"hockey\", \"browser\", \"food\"]:\n",
    "for label in top_labels(image_df):\n",
    "    plot_hits(top_k(image_df, label, k=8), label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "## Synthesis\n",
    "\n",
    "Both pipelines are compared on the same viable-label slice so the comparison is controlled:\n",
    "\n",
    "- **OCR/Text**: TF-IDF + classifier on OCR transcripts\n",
    "- **CLIP/Image**: classifier on CLIP embeddings\n",
    "\n",
    "If CLIP materially outperforms OCR, CLIP becomes the default basis for clustering and labeling operations. OCR may remain useful as side information, but not as the main driver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "### side-by-side per-label F1 on viable set\n",
    "re-scope both models to the same viable split for a fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_binarizer = MultiLabelBinarizer(classes=viable)\n",
    "syn_targets = syn_binarizer.fit_transform([r[\"categories\"] for r in sub_rows])\n",
    "tr, te = ensure_label_coverage(syn_targets, *split_indices(len(sub_rows)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "### text winner config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_text_vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(2, 4), max_df=0.95)\n",
    "syn_text_feat = syn_text_vec.fit_transform(sub_ocr)\n",
    "syn_text_clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "syn_text_clf.fit(syn_text_feat[tr], syn_targets[tr])\n",
    "syn_text_preds = (syn_text_clf.predict_proba(syn_text_feat[te]) >= THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "### image clip embeddings scoped to viable subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_clip = clip_embeddings[viable_idx]\n",
    "syn_image_clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "syn_image_clf.fit(sub_clip[tr], syn_targets[tr])\n",
    "syn_image_preds = (syn_image_clf.predict_proba(sub_clip[te]) >= THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### per-label F1 for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "text_f1s = f1_score(syn_targets[te], syn_text_preds, average=None, zero_division=0)\n",
    "image_f1s = f1_score(syn_targets[te], syn_image_preds, average=None, zero_division=0)\n",
    "support = syn_targets[te].sum(axis=0)\n",
    "\n",
    "cmp = pd.DataFrame(\n",
    "    {\n",
    "        \"text_f1\": text_f1s,\n",
    "        \"image_f1\": image_f1s,\n",
    "        \"delta\": image_f1s - text_f1s,\n",
    "        \"support\": support.astype(int),\n",
    "    },\n",
    "    index=viable,\n",
    ").sort_values(\"delta\", ascending=False)\n",
    "\n",
    "text_micro = f1_score(syn_targets[te], syn_text_preds, average=\"micro\", zero_division=0)\n",
    "text_macro = f1_score(syn_targets[te], syn_text_preds, average=\"macro\", zero_division=0)\n",
    "image_micro = f1_score(syn_targets[te], syn_image_preds, average=\"micro\", zero_division=0)\n",
    "image_macro = f1_score(syn_targets[te], syn_image_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "print(cmp.to_string(float_format=\"{:.3f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "The results are clear: **CLIP features substantially outperform OCR-text features** on the same viable-label split: \n",
    "\n",
    "Text reaches micro/macro F1 of 0.349/0.105, while image reaches 0.674/0.584 (F1 ranges from 0 to 1, where 1 is perfect). \n",
    "  \n",
    "The key signal is macro F1, because it weights each label equally and therefore reflects whether the model works beyond only frequent categories. A macro score of 0.105 indicates OCR is failing on most labels, even if it captures a few common textual patterns; by contrast, CLIP’s 0.584 shows broad, usable separability across the label set. \n",
    "\n",
    "The micro improvement (0.349 to 0.674) confirms this gain is not only per-label but also strong on aggregate predictions. Relatively, the gains in CLIP are substantial:\n",
    "\n",
    "- micro: 0.674 vs 0.349 (+0.325, about 1.9x)\n",
    "- macro: 0.584 vs 0.105 (+0.479, about 5.6x)\n",
    "\n",
    "Therefore, using CLIP embeddings as the primary representation for clustering and label acceleration in the gallery workflow is the cromulent choice. OCR may be useful as auxiliary metadata rather than the main classification signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "Let's also check to see if OCR may provide speed improvements.\n",
    "\n",
    "### Performance\n",
    "\n",
    "Estimate total runtime and compare CLIP vs OCR throughput on the same subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def probe_embed_rate(all_image_paths, embed_fn, n=50):\n",
    "    probe = all_image_paths[: min(n, len(all_image_paths))]\n",
    "    t0 = time.perf_counter()\n",
    "    _ = embed_fn(probe)\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    rate = len(probe) / elapsed if elapsed else float(\"inf\")\n",
    "    eta = len(all_image_paths) / rate if rate else float(\"inf\")\n",
    "    print(f\"CLIP: {len(probe)} images in {elapsed:.1f}s - {rate:.1f} it/s\")\n",
    "    print(f\"estimated CLIP full run: {eta / 60:.1f} min ({len(all_image_paths)} images)\")\n",
    "    return {\"n\": len(probe), \"elapsed\": elapsed, \"rate\": rate, \"eta\": eta}\n",
    "\n",
    "\n",
    "def probe_ocr_rate(all_image_paths, ocr_fn, n=50, psm=6, oem=3):\n",
    "    probe = all_image_paths[: min(n, len(all_image_paths))]\n",
    "    skipped = 0\n",
    "    t0 = time.perf_counter()\n",
    "    for path in tqdm(probe, desc=\"ocr-probe\"):\n",
    "        try:\n",
    "            _ = ocr_fn(path, psm=psm, oem=oem)\n",
    "        except OSError:\n",
    "            skipped += 1\n",
    "    elapsed = time.perf_counter() - t0\n",
    "    processed = len(probe) - skipped\n",
    "    rate = processed / elapsed if elapsed else float(\"inf\")\n",
    "    eta = len(all_image_paths) / rate if rate else float(\"inf\")\n",
    "    print(f\"OCR:  {processed} images in {elapsed:.1f}s - {rate:.1f} it/s (skipped={skipped})\")\n",
    "    print(f\"estimated OCR full run: {eta / 60:.1f} min ({len(all_image_paths)} images)\")\n",
    "    return {\n",
    "        \"n\": len(probe),\n",
    "        \"processed\": processed,\n",
    "        \"skipped\": skipped,\n",
    "        \"elapsed\": elapsed,\n",
    "        \"rate\": rate,\n",
    "        \"eta\": eta,\n",
    "    }\n",
    "\n",
    "\n",
    "def compare_clip_vs_ocr(all_image_paths, n=50):\n",
    "    clip_stats = probe_embed_rate(all_image_paths, embed_images_clip, n=n)\n",
    "    ocr_stats = probe_ocr_rate(all_image_paths, ocr_image, n=n)\n",
    "    if ocr_stats[\"rate\"] == 0:\n",
    "        print(\"clip/ocr speed ratio: inf (ocr rate is zero)\")\n",
    "    else:\n",
    "        ratio = clip_stats[\"rate\"] / ocr_stats[\"rate\"]\n",
    "        print(f\"clip/ocr speed ratio: {ratio:.2f}x\")\n",
    "    return {\"clip\": clip_stats, \"ocr\": ocr_stats}\n",
    "\n",
    "\n",
    "perf = compare_clip_vs_ocr(all_image_paths, n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "These results are clear: CLIP is 3x's to 7x's more efficient than tessarct text extraction (on CPU). \n",
    "\n",
    "## CLIP Clusters for Labeling Workflow\n",
    "\n",
    "Because CLIP is stronger on the labeled slice, embeddings are computed/cached for the full corpus and clustered. \n",
    "\n",
    "The cluster view may then be used to accelerate labeling in `www/`:\n",
    "\n",
    "1. Open a cluster slice in gallery mode\n",
    "2. Multi-select obvious in-cluster matches\n",
    "3. Excise outliers with quick deselection/click edits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "### Compute and Save all embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDINGS_PATH = ROOT / \"data/embeddings/clip_full.pkl\"\n",
    "EMBEDDINGS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if EMBEDDINGS_PATH.exists():\n",
    "    all_clip_embeddings = joblib.load(EMBEDDINGS_PATH)\n",
    "    n_cached = all_clip_embeddings.shape[0]\n",
    "else:\n",
    "    all_clip_embeddings = None\n",
    "    n_cached = 0\n",
    "\n",
    "n_paths = len(all_image_paths)\n",
    "assert n_cached <= n_paths, f\"cache has more rows than paths: {n_cached} > {n_paths}\"\n",
    "\n",
    "if n_cached < n_paths:\n",
    "    new_embeddings = embed_images_clip(all_image_paths[n_cached:])\n",
    "    all_clip_embeddings = (\n",
    "        new_embeddings\n",
    "        if all_clip_embeddings is None\n",
    "        else np.vstack([all_clip_embeddings, new_embeddings])\n",
    "    )\n",
    "    joblib.dump(all_clip_embeddings, EMBEDDINGS_PATH)\n",
    "\n",
    "print(all_clip_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "#### UMAP Dimensionality Reduction\n",
    "\n",
    "Reduce high-dimensional CLIP embeddings into a 2D representation using UMAP for visualization (heatmap). The algorithm preserves local neighborhood structure based on cosine similarity, with `n_neighbors=15` controlling locality and `min_dist=0.1` allowing relatively compact clusters. The resulting embedding (`embedding2d`) contains one 2D point per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_components=2,\n",
    "    n_neighbors=15,\n",
    "    min_dist=0.1,\n",
    "    metric=\"cosine\",\n",
    "    random_state=SEED,\n",
    ")\n",
    "embedding2d = reducer.fit_transform(all_clip_embeddings)\n",
    "print(f\"reduced: {embedding2d.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "#### Density-Based Clustering with HDBSCAN\n",
    "\n",
    "Apply HDBSCAN to the 2D UMAP embedding to identify clusters based on point density. The algorithm groups regions of sufficient density while labeling sparse regions as noise (`-1`).\n",
    "\n",
    "`min_cluster_size=10` defines the smallest allowable cluster, and `min_samples=5` controls how conservatively dense regions are defined. The output `cluster_labels` assigns each point either a cluster ID or noise. The summary reports the number of detected clusters, the number of noise points, and the total samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10, min_samples=5, metric=\"euclidean\")\n",
    "cluster_labels = clusterer.fit_predict(embedding2d)\n",
    "\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = (cluster_labels == -1).sum()\n",
    "print(f\"clusters: {n_clusters}  noise points: {n_noise}  total: {len(cluster_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### Cluster Visualization in UMAP Space\n",
    "\n",
    "Visualize the 2D UMAP embedding with points colored according to HDBSCAN cluster assignments. Each color corresponds to a density-based cluster detected in the embedding space, while noise points (`-1`) are shown in light grey.\n",
    "\n",
    "The legend reports cluster IDs and their sample counts. The title reflects the total number of images and the number of clusters identified by HDBSCAN (excluding noise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 12))\n",
    "\n",
    "unique_clusters = sorted(set(cluster_labels))\n",
    "cmap = plt.cm.get_cmap(\"tab20\", len(unique_clusters))\n",
    "\n",
    "for i, cid in enumerate(unique_clusters):\n",
    "    mask = cluster_labels == cid\n",
    "    label = f\"noise ({mask.sum()})\" if cid == -1 else f\"cluster {cid} ({mask.sum()})\"\n",
    "    color = \"lightgrey\" if cid == -1 else cmap(i)\n",
    "    ax.scatter(embedding2d[mask, 0], embedding2d[mask, 1], s=6, color=color, label=label, alpha=0.6)\n",
    "\n",
    "ax.legend(bbox_to_anchor=(1.01, 1), loc=\"upper left\", fontsize=7, markerscale=2)\n",
    "ax.set_title(f\"UMAP - {len(all_image_paths)} images, {n_clusters} clusters\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "####  inspect a cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_cluster(cid, k=12):\n",
    "    indices = [i for i, c in enumerate(cluster_labels) if c == cid]\n",
    "    sample = np.random.RandomState(SEED).choice(indices, size=min(k, len(indices)), replace=False)\n",
    "    hits = [{\"path\": all_image_paths[i], \"score\": clusterer.probabilities_[i]} for i in sample]\n",
    "    plot_hits(hits, columns=4, label=f\"cluster {cid} - n={len(indices)}\")\n",
    "\n",
    "\n",
    "# inspect_cluster(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "#### gallery sweep, sample from every cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_summary(k=8, columns=4, skip_noise=True):\n",
    "    unique = sorted(c for c in set(cluster_labels) if not (skip_noise and c == -1))\n",
    "    for cid in unique:\n",
    "        indices = [i for i, c in enumerate(cluster_labels) if c == cid]\n",
    "        sample = np.random.RandomState(SEED).choice(\n",
    "            indices, size=min(k, len(indices)), replace=False\n",
    "        )\n",
    "        hits = [{\"path\": all_image_paths[i], \"score\": clusterer.probabilities_[i]} for i in sample]\n",
    "        plot_hits(hits, columns=columns, label=f\"cluster {cid} — n={len(indices)}\")\n",
    "\n",
    "\n",
    "# cluster_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63",
   "metadata": {},
   "source": [
    "##  Jaccard Similarity of Cluster OCR Vocabulary vs Manual Labels\n",
    "\n",
    "This is an interpretability aid for clusters.\n",
    "\n",
    "1. Build token sets from OCR text per cluster\n",
    "2. Build token sets from OCR text per manual label (from labeled slice)\n",
    "3. Compute Jaccard overlap\n",
    "\n",
    "Use this as a **candidate label hint** for each cluster. High overlap suggests likely affinity, but final assignment still comes from visual inspection in that labeler gallery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "### 1) map each image path to its cluster id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(all_image_paths) == len(cluster_labels)\n",
    "path_to_cluster = {all_image_paths[i]: int(cluster_labels[i]) for i in range(len(all_image_paths))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66",
   "metadata": {},
   "source": [
    "### 2) token sets per cluster: union of all OCR words across member images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return set(text.lower().split())\n",
    "\n",
    "\n",
    "cluster_vocab = {}\n",
    "for i, path in enumerate(all_image_paths):\n",
    "    cid = cluster_labels[i]\n",
    "    txt_path = ROOT / \"data/ocr\" / path.parent.name / (path.stem + \".txt\")\n",
    "    if not txt_path.exists():\n",
    "        continue\n",
    "    tokens = tokenize(txt_path.read_text())\n",
    "    cluster_vocab.setdefault(cid, set()).update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "### 3) token sets per label: union of OCR words across labeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_vocab = {}\n",
    "for r, ocr in zip(rows, text_ocr):\n",
    "    tokens = tokenize(ocr)\n",
    "    for label in r[\"categories\"]:\n",
    "        label_vocab.setdefault(label, set()).update(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "### 4) compare Jaccard similarity: cluster vs label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(a, b):\n",
    "    if not a or not b:\n",
    "        return 0.0\n",
    "    return len(a & b) / len(a | b)\n",
    "\n",
    "\n",
    "unique_clusters = sorted(c for c in set(cluster_labels) if c != -1)\n",
    "labels_sorted = sorted(label_vocab)\n",
    "\n",
    "jac = pd.DataFrame(\n",
    "    [\n",
    "        [\n",
    "            jaccard(cluster_vocab.get(cid, set()), label_vocab.get(label, set()))\n",
    "            for label in labels_sorted\n",
    "        ]\n",
    "        for cid in unique_clusters\n",
    "    ],\n",
    "    index=[f\"c{cid}\" for cid in unique_clusters],\n",
    "    columns=labels_sorted,\n",
    ")\n",
    "\n",
    "# jac"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "### 5) show top 5 label matches per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_match_table(jac, cluster_labels, unique_clusters, top_n=5):\n",
    "    rows = {}\n",
    "    for cid in unique_clusters:\n",
    "        n = (cluster_labels == cid).sum()\n",
    "        top = jac.loc[f\"c{cid}\"].nlargest(top_n)\n",
    "        row = {\"n\": n}\n",
    "        for rank, (label, score) in enumerate(top.items(), 1):\n",
    "            row[f\"top{rank}\"] = f\"{label} ({score:.3f})\"\n",
    "        rows[f\"c{cid}\"] = row\n",
    "\n",
    "    return pd.DataFrame(rows).T\n",
    "\n",
    "\n",
    "cluster_match_table(jac, cluster_labels, unique_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "### 6) heatmap: column filter and drop labels when no cluster scores highly\n",
    "\n",
    "This makes it clear, numerically, how diffuse the labels are for each cluster. It is too close to call if top2 is statistically better than he top3 label for c0 (the c0 cluster is pictures of people's faces with little to no OCR data)\n",
    "\n",
    "It's better to get a visual on the diffussion, which is where a heatmap is the right show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.10\n",
    "VMAX_PCTILE = 99  # prevents outlier bleaching\n",
    "\n",
    "active_cols = jac.columns[jac.max(axis=0) >= THRESHOLD]\n",
    "J = jac[active_cols].copy()\n",
    "\n",
    "cluster_sizes = {cid: int((cluster_labels == cid).sum()) for cid in unique_clusters}\n",
    "dominant = J.idxmax(axis=1)\n",
    "J.index = [\n",
    "    f\"c{i.lstrip('c'):>2} n={cluster_sizes[int(i.lstrip('c'))]:>3} [{dominant[i]}]\" for i in J.index\n",
    "]\n",
    "\n",
    "# J.shape, len(active_cols)\n",
    "# J.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "### 7) plot filtered OCR-Jaccard clustermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmax = np.percentile(J.values, VMAX_PCTILE)\n",
    "\n",
    "cg = sns.clustermap(\n",
    "    J,\n",
    "    method=\"ward\",\n",
    "    metric=\"euclidean\",\n",
    "    cmap=\"YlOrRd\",\n",
    "    vmin=0,\n",
    "    vmax=vmax,\n",
    "    figsize=(max(14, len(active_cols) * 0.45), max(12, len(J) * 0.22)),\n",
    "    linewidths=0.2,\n",
    "    linecolor=\"#e0e0e0\",\n",
    "    xticklabels=True,\n",
    "    yticklabels=True,\n",
    "    dendrogram_ratio=(0.10, 0.06),\n",
    "    cbar_pos=(0.01, 0.84, 0.02, 0.12),\n",
    "    cbar_kws={\"label\": f\"Jaccard (capped p{VMAX_PCTILE})\"},\n",
    ")\n",
    "plt.setp(cg.ax_heatmap.get_xticklabels(), rotation=45, ha=\"right\", fontsize=8)\n",
    "plt.setp(cg.ax_heatmap.get_yticklabels(), fontsize=7.5)\n",
    "cg.ax_heatmap.set_xlabel(\"Label\", fontsize=10)\n",
    "cg.ax_heatmap.set_ylabel(\"\")\n",
    "cg.ax_col_dendrogram.set_title(\n",
    "    f\"Cluster × Label — Ward/Euclidean clustermap  (threshold={THRESHOLD})\", fontsize=11\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# J.shape, vmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78",
   "metadata": {},
   "source": [
    "## CLIP cluster vote transfer\n",
    "\n",
    "`rows` holds manual labels. `cluster_labels` is indexed by `all_image_paths`.\n",
    "This section builds `mapped` (labeled row -> cluster id), then `clip_vote` (cluster x label share).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "### 1) set parameters and aligned slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIM_MIN = 0.98\n",
    "MIN_CLUSTER_ROWS = 2\n",
    "FOCUS_LABELS = 40\n",
    "FOCUS_CLUSTERS = 48\n",
    "\n",
    "labeled = clip_embeddings.astype(float)\n",
    "full = all_clip_embeddings.astype(float)\n",
    "full_clusters = np.asarray(cluster_labels, dtype=int)\n",
    "\n",
    "# (len(rows), labeled.shape, full.shape, full_clusters.shape)\n",
    "# full_clusters[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "### 2) build row index for vectorized lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_idx = np.arange(len(rows))\n",
    "\n",
    "# row_idx[:10]\n",
    "# len(row_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "### 3) compute nearest non-self CLIP match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_norm = np.linalg.norm(labeled, axis=1, keepdims=True)\n",
    "full_norm = np.linalg.norm(full, axis=1, keepdims=True)\n",
    "labeled_unit = labeled / np.clip(labeled_norm, 1e-12, None)\n",
    "full_unit = full / np.clip(full_norm, 1e-12, None)\n",
    "sim = labeled_unit @ full_unit.T\n",
    "\n",
    "row_idx = np.arange(len(rows))\n",
    "nn_idx = sim.argmax(axis=1)\n",
    "nn_sim = sim[row_idx, nn_idx]\n",
    "mapped_cid = full_clusters[nn_idx]\n",
    "\n",
    "# (sim.shape, row_idx.shape, nn_idx.shape, nn_sim.shape, mapped_cid.shape)\n",
    "# nn_sim[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85",
   "metadata": {},
   "source": [
    "### 4) keep valid matches and aggregate cluster vote share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped = [\n",
    "    (i, int(cid))\n",
    "    for i, cid in enumerate(mapped_cid)\n",
    "    if int(cid) != -1 and float(nn_sim[i]) >= SIM_MIN\n",
    "]\n",
    "\n",
    "cluster_sizes = Counter(cid for _, cid in mapped)\n",
    "label_counts = {}\n",
    "for row_idx, cid in mapped:\n",
    "    label_counts.setdefault(cid, Counter()).update(rows[row_idx][\"categories\"])\n",
    "\n",
    "labels = sorted({label for counts in label_counts.values() for label in counts})\n",
    "clip_vote = pd.DataFrame(0.0, index=sorted(label_counts), columns=labels)\n",
    "for cid, counts in label_counts.items():\n",
    "    denom = cluster_sizes[cid]\n",
    "    for label, count in counts.items():\n",
    "        clip_vote.loc[cid, label] = count / denom\n",
    "\n",
    "# len(mapped)\n",
    "# clip_vote.shape\n",
    "# pd.Series(cluster_sizes).sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "### 5) filter matrix before clustering\n",
    "\n",
    "Keep only high-signal rows/columns so the clustermap reflects dominant structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "active_cols = clip_vote.columns[clip_vote.max(axis=0) >= THRESHOLD]\n",
    "col_score = clip_vote[active_cols].max(axis=0).sort_values(ascending=False)\n",
    "focus_cols = col_score.head(FOCUS_LABELS).index\n",
    "\n",
    "row_score = clip_vote[focus_cols].max(axis=1).sort_values(ascending=False)\n",
    "focus_rows = [\n",
    "    cid\n",
    "    for cid in row_score.index\n",
    "    if cluster_sizes[cid] >= MIN_CLUSTER_ROWS and row_score.loc[cid] > 0\n",
    "][:FOCUS_CLUSTERS]\n",
    "\n",
    "C = clip_vote.loc[focus_rows, focus_cols].copy()\n",
    "dominant = C.idxmax(axis=1)\n",
    "C.index = [f\"c{cid:>3} n={cluster_sizes[cid]:>2} [{dominant.loc[cid]}]\" for cid in C.index]\n",
    "\n",
    "# (len(active_cols), len(focus_cols), len(focus_rows), C.shape)\n",
    "# C.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "### 6) plot focused CLIP vote clustermap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmax = np.percentile(C.values, VMAX_PCTILE)\n",
    "fig_w = max(14, C.shape[1] * 0.50)\n",
    "fig_h = max(10, C.shape[0] * 0.28 + 2)\n",
    "cg = sns.clustermap(\n",
    "    C,\n",
    "    method=\"ward\",\n",
    "    metric=\"euclidean\",\n",
    "    cmap=\"YlOrRd\",\n",
    "    vmin=0,\n",
    "    vmax=vmax,\n",
    "    figsize=(fig_w, fig_h),\n",
    "    linewidths=0.2,\n",
    "    linecolor=\"#e0e0e0\",\n",
    "    xticklabels=True,\n",
    "    yticklabels=True,\n",
    "    dendrogram_ratio=(0.10, 0.06),\n",
    "    cbar_pos=(0.01, 0.82, 0.02, 0.14),\n",
    "    cbar_kws={\"label\": f\"CLIP vote share (capped p{VMAX_PCTILE})\"},\n",
    ")\n",
    "# check: (C.shape, vmax, fig_w, fig_h)\n",
    "plt.setp(cg.ax_heatmap.get_xticklabels(), rotation=45, ha=\"right\", fontsize=8)\n",
    "plt.setp(cg.ax_heatmap.get_yticklabels(), fontsize=7.5)\n",
    "cg.ax_heatmap.set_xlabel(\"Label\", fontsize=10)\n",
    "cg.ax_heatmap.set_ylabel(\"\")\n",
    "cg.ax_col_dendrogram.set_title(\n",
    "    \"CLIP Cluster x Label Vote (focused) — Ward/Euclidean\",\n",
    "    fontsize=11,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91",
   "metadata": {},
   "source": [
    "## CLIP vote vs OCR-Jaccard on the same mapped rows\n",
    "\n",
    "Evaluation protocol:\n",
    "- Use the same `mapped` rows from the CLIP step\n",
    "- Leave-one-out style prediction per row\n",
    "- Compare prediction sets with micro/macro F1 and hit-rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "### 1) prepare shared evaluation rows and grouped OCR tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_rows = [(row_idx, cid, set(rows[row_idx][\"categories\"])) for row_idx, cid in mapped]\n",
    "tokens = [tokenize(text_ocr[row_idx]) for row_idx, _, _ in eval_rows]\n",
    "\n",
    "by_cluster = {}\n",
    "for pos, (_, cid, _) in enumerate(eval_rows):\n",
    "    by_cluster.setdefault(cid, []).append(pos)\n",
    "\n",
    "mapped_cluster_vocab = {}\n",
    "for pos, (_, cid, _) in enumerate(eval_rows):\n",
    "    mapped_cluster_vocab.setdefault(cid, set()).update(tokens[pos])\n",
    "\n",
    "# len(eval_rows), len(tokens), len(by_cluster), len(mapped_cluster_vocab)\n",
    "# pd.Series([len(v) for v in by_cluster.values()]).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94",
   "metadata": {},
   "source": [
    "### 2) define CLIP vote baseline\n",
    "`clip_preds` baseline: for each row, collect labels from peer rows in the same cluster and keep labels that exceed `min_share`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_preds(min_peers=2, top_k=3, min_share=0.40):\n",
    "    preds = []\n",
    "    for pos, (_, cid, _) in enumerate(eval_rows):\n",
    "        peers = [j for j in by_cluster[cid] if j != pos]\n",
    "        if len(peers) < min_peers:\n",
    "            preds.append(set())\n",
    "            continue\n",
    "        counts = Counter(label for j in peers for label in eval_rows[j][2])\n",
    "        ranked = sorted(counts.items(), key=lambda item: item[1], reverse=True)[:top_k]\n",
    "        preds.append({label for label, count in ranked if (count / len(peers)) >= min_share})\n",
    "    return preds\n",
    "\n",
    "\n",
    "# preview_clip = clip_preds()\n",
    "# [sorted(list(p)) for p in preview_clip[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "### 3) define OCR-Jaccard baseline\n",
    "\n",
    "`ocr_preds` baseline: build label-specific token vocab from other rows, compare each label vocab with mapped-cluster OCR vocab (same eval rows) using Jaccard similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_preds(top_k=3, min_score=0.10):\n",
    "    preds = []\n",
    "    for pos, (_, cid, _) in enumerate(eval_rows):\n",
    "        label_vocab = {}\n",
    "        for j, (_, _, true_labels) in enumerate(eval_rows):\n",
    "            if j == pos:\n",
    "                continue\n",
    "            for label in true_labels:\n",
    "                label_vocab.setdefault(label, set()).update(tokens[j])\n",
    "        c_tokens = mapped_cluster_vocab.get(cid, set())\n",
    "        ranked = sorted(\n",
    "            ((label, jaccard(c_tokens, tks)) for label, tks in label_vocab.items()),\n",
    "            key=lambda item: item[1],\n",
    "            reverse=True,\n",
    "        )[:top_k]\n",
    "        preds.append({label for label, score in ranked if score >= min_score})\n",
    "    return preds\n",
    "\n",
    "\n",
    "# preview_ocr = ocr_preds()\n",
    "# [sorted(list(p)) for p in preview_ocr[:5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "### 4) define scoring function\n",
    "\n",
    "`score` converts label sets to binary matrices and reports coverage, hit-rate, micro/macro F1, and per-label F1 deltas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(method, preds):\n",
    "    y_true_list = [sorted(labels) for _, _, labels in eval_rows]\n",
    "    y_pred_list = [sorted(p) for p in preds]\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y_true = mlb.fit_transform(y_true_list)\n",
    "    y_pred = mlb.transform(y_pred_list)\n",
    "    metrics = {\n",
    "        \"method\": method,\n",
    "        \"n\": len(eval_rows),\n",
    "        \"coverage\": sum(len(p) > 0 for p in preds) / len(eval_rows),\n",
    "        \"hit_rate\": sum(len(p & t) > 0 for p, t in zip(preds, [r[2] for r in eval_rows]))\n",
    "        / len(eval_rows),\n",
    "        \"micro_f1\": f1_score(y_true, y_pred, average=\"micro\", zero_division=0),\n",
    "        \"macro_f1\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"mean_pred_labels\": float(np.mean([len(p) for p in preds])),\n",
    "    }\n",
    "    per_label = pd.Series(\n",
    "        f1_score(y_true, y_pred, average=None, zero_division=0),\n",
    "        index=mlb.classes_,\n",
    "        dtype=float,\n",
    "    )\n",
    "    support = pd.Series(y_true.sum(axis=0), index=mlb.classes_, dtype=float)\n",
    "    return metrics, per_label, support\n",
    "\n",
    "\n",
    "# debug_metrics, debug_f1, debug_support = score(\"debug\", clip_preds())\n",
    "# debug_metrics\n",
    "# debug_support.sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "### 5) run both predictors on the same rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_clip = clip_preds()\n",
    "pred_ocr = ocr_preds()\n",
    "\n",
    "# (len(pred_clip), len(pred_ocr))\n",
    "# (pred_clip[:3], pred_ocr[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "### 6) compute aggregate metrics and per-label delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_clip, f1_clip, support = score(\"CLIP vote\", pred_clip)\n",
    "m_ocr, f1_ocr, _ = score(\"OCR-Jaccard\", pred_ocr)\n",
    "\n",
    "cmp = pd.DataFrame([m_clip, m_ocr]).set_index(\"method\")\n",
    "print(\n",
    "    cmp[[\"n\", \"coverage\", \"hit_rate\", \"micro_f1\", \"macro_f1\", \"mean_pred_labels\"]].to_string(\n",
    "        float_format=\"{:.3f}\".format\n",
    "    )\n",
    ")\n",
    "\n",
    "delta = (f1_clip - f1_ocr).rename(\"clip_minus_ocr\")\n",
    "label_cmp = pd.concat(\n",
    "    [f1_clip.rename(\"clip_f1\"), f1_ocr.rename(\"ocr_f1\"), delta, support.rename(\"support\")],\n",
    "    axis=1,\n",
    ").sort_values(\"clip_minus_ocr\", ascending=False)\n",
    "\n",
    "# cmp\n",
    "# label_cmp.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104",
   "metadata": {},
   "source": [
    "#### top labels where CLIP > OCR-Jaccard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_cmp.head(10).to_string(float_format=\"{:.3f}\".format))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106",
   "metadata": {},
   "source": [
    "#### top labels where OCR-Jaccard >= CLIP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_cmp.tail(10).to_string(float_format=\"{:.3f}\".format))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwatch(0)\n",
    "# 129.23865489498712 # (seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-activity (.venv)",
   "language": "python",
   "name": "image-activity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
