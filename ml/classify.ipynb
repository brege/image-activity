{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Screenshot predictions\n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "**Label**: manually assigned categories from the `www/` labeling tool, e.g. \"hockey\", \"food\", etc. These are the training targets for sklearn classifiers. These are semantic and contextual, not perceptual.\n",
    "\n",
    "**Class**: ResNet/ImageNet's 1000 output categories. \"ice hockey\", \"puck\", \"hot dog\", \"broccoli\". These are perceptual and specific.\n",
    "\n",
    "**Embedding**: the 512-dim vector ResNet produces after stripping `fc`. A geometric representation of visual structure, no semantic meaning attached.\n",
    "\n",
    "**Feature**: a dimension or region of signal used as input to a model. Embeddings are features. TF-IDF weights are features. Raw pixels are not.\n",
    "\n",
    "**Prediction**: the output of a classifier against a defined set of targets. Your sklearn classifiers produce predictions against your labels.\n",
    "\n",
    "**Probability / score**: the confidence value attached to a prediction.\n",
    "\n",
    "## Classification Scheme\n",
    "\n",
    "Instead of using the 512-dim embedding as features, we can run the full ResNet forward pass and take the 1000-dim class probability vector (\"this image is 40% white ice, 30% crowds, 15% faces\") and use *that* as features into a sklearn classifier.\n",
    "\n",
    "The ImageNet classes become a perceptual vocabulary that your label classifier reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Set pathing and shared data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_root(marker=\"data/labels.jsonl\"):\n",
    "    for p in (Path.cwd(), *Path.cwd().parents):\n",
    "        if (p / marker).exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(marker)\n",
    "\n",
    "\n",
    "ROOT = find_root()\n",
    "LABELS_PATH = ROOT / \"data/labels.jsonl\"\n",
    "\n",
    "rows = [json.loads(line) for line in LABELS_PATH.read_text().splitlines() if line.strip()]\n",
    "paths = [ROOT / r[\"input_path\"] for r in rows]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Helpers\n",
    "- `make_df` wraps classifier output into a labeled DataFrame indexed by filename.\n",
    "- `top_k` slices that DataFrame by label column and returns ranked hits as a list\n",
    "of `{path, score}` dicts  the currency everything else operates on.\n",
    "- `top_labels` picks the *n* highest-mass columns by summing probability across\n",
    "all images. This is useful for a first-pass survey of what the model is confident about.\n",
    "- `print_hits` and `plot_hits` produce table and gallery of images and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "TEST_SIZE = 0.2\n",
    "THRESHOLD = 0.2\n",
    "\n",
    "\n",
    "def make_df(probs, binarizer):\n",
    "    return pd.DataFrame(probs, columns=binarizer.classes_, index=[p.name for p in paths])\n",
    "\n",
    "\n",
    "def top_k(df, label, k=12):\n",
    "    scores = df[label].to_numpy()\n",
    "    order = np.argsort(scores)[::-1][:k]\n",
    "    return [{\"path\": paths[i], \"score\": scores[i]} for i in order]\n",
    "\n",
    "\n",
    "def top_labels(df, n=5):\n",
    "    return df.sum().nlargest(n).index.tolist()\n",
    "\n",
    "\n",
    "def print_hits(hits):\n",
    "    for h in hits:\n",
    "        print(f\"{h['score']:.4f}  {h['path'].parent.name}/{h['path'].name}\")\n",
    "\n",
    "\n",
    "def plot_hits(hits, columns=4, label=\"\"):\n",
    "    rows_ = (len(hits) + columns - 1) // columns\n",
    "    fig, axes = plt.subplots(rows_, columns, figsize=(3 * columns, 3 * rows_))\n",
    "    axes = np.array(axes).reshape(-1)\n",
    "    for ax, hit in zip(axes, hits):\n",
    "        ax.imshow(Image.open(hit[\"path\"]))\n",
    "        ax.set_title(f\"{hit['score']:.3f}\\n{hit['path'].name}\")\n",
    "        ax.axis(\"off\")\n",
    "    for ax in axes[len(hits) :]:\n",
    "        ax.axis(\"off\")\n",
    "    if label:\n",
    "        fig.suptitle(label)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def split_indices(n):\n",
    "    idx = np.random.RandomState(SEED).permutation(n)\n",
    "    cut = int(n * (1 - TEST_SIZE))\n",
    "    return idx[:cut], idx[cut:]\n",
    "\n",
    "\n",
    "def ensure_label_coverage(targets, train_idx, test_idx):\n",
    "    train_set, test_set = set(train_idx.tolist()), set(test_idx.tolist())\n",
    "    for col in range(targets.shape[1]):\n",
    "        if targets[list(train_set), col].sum() > 0:\n",
    "            continue\n",
    "        candidates = [i for i in test_set if targets[i, col] == 1]\n",
    "        if candidates:\n",
    "            test_set.remove(candidates[0])\n",
    "            train_set.add(candidates[0])\n",
    "    return np.array(sorted(train_set)), np.array(sorted(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Text model\n",
    "\n",
    "Text pipeline bundle \n",
    "\n",
    "1. vectorizer\n",
    "2. classifier\n",
    "3. and binarizer\n",
    "   \n",
    "Then\n",
    "\n",
    "4. resolves the OCR transcript for each image from `data/ocr/`\n",
    "5. transforms the corpus into TF-IDF features\n",
    "6. and scores every document against all labels\n",
    "\n",
    "Produces `text_df`: one row per image by label column vectors. Elements are class probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def ocr_path(r):\n",
    "    img = Path(r[\"input_path\"])\n",
    "    return ROOT / \"data/ocr\" / img.parent.name / (img.stem + \".txt\")\n",
    "\n",
    "\n",
    "text_ocr = [ocr_path(r).read_text().lower() for r in rows]\n",
    "text_binarizer = MultiLabelBinarizer()\n",
    "text_targets = text_binarizer.fit_transform([r[\"categories\"] for r in rows])\n",
    "\n",
    "text_vec = TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5), max_df=0.95)\n",
    "text_features = text_vec.fit_transform(text_ocr)\n",
    "\n",
    "print(\n",
    "    \"samples:\",\n",
    "    len(rows),\n",
    "    \"labels:\",\n",
    "    len(text_binarizer.classes_),\n",
    "    \"features:\",\n",
    "    text_features.shape[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "### Training on OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, te = ensure_label_coverage(text_targets, *split_indices(len(rows)))\n",
    "\n",
    "text_clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "text_clf.fit(text_features[tr], text_targets[tr])\n",
    "\n",
    "text_probs_test = text_clf.predict_proba(text_features[te])\n",
    "text_preds_test = (text_probs_test >= THRESHOLD).astype(int)\n",
    "\n",
    "print(\"micro:\", f1_score(text_targets[te], text_preds_test, average=\"micro\", zero_division=0))\n",
    "print(\"macro:\", f1_score(text_targets[te], text_preds_test, average=\"macro\", zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "### Score and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_probs = text_clf.predict_proba(text_features)\n",
    "text_df = make_df(text_probs, text_binarizer)\n",
    "\n",
    "joblib.dump(\n",
    "    {\"vectorizer\": text_vec, \"classifier\": text_clf, \"binarizer\": text_binarizer},\n",
    "    ROOT / \"data/models/text.joblib\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df[top_labels(text_df)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"hockey\", \"browser\", \"food\"]:\n",
    "    plot_hits(top_k(text_df, label, k=8), label=label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## Image model\n",
    "\n",
    "Rather than training a vision model from scratch, we use ResNet18 as a feature extractor. ResNet18 was pretrained on ImageNet and has learned general visual structure: edges, textures, shapes, compositions. \n",
    "\n",
    "The final layer of ResNet18 is a fully connected layer (`fc`) that maps learned features to a probability distribution over ImageNet's 1000 classes. \n",
    "\n",
    "We replace it with an identity function, which passes its input through unchanged. This stops the network before it commits to ImageNet categories and gives us the raw 512-dimensional feature vector instead. \n",
    "\n",
    "Those embeddings were used to train the sklearn classifier in the bundle, so at inference time we run the same extraction and hand the vectors to it. The model never sees pixel values directly. Only ResNet's learned representation of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = models.ResNet18_Weights.DEFAULT\n",
    "resnet = models.resnet18(weights=weights)\n",
    "resnet.fc = torch.nn.Identity()\n",
    "resnet.eval()\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "\n",
    "def embed_images(image_paths):\n",
    "    vectors = []\n",
    "    for path in image_paths:\n",
    "        with Image.open(path) as img:\n",
    "            tensor = preprocess(img.convert(\"RGB\")).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            vectors.append(resnet(tensor).cpu().numpy().squeeze(0))\n",
    "    return np.vstack(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_binarizer = MultiLabelBinarizer()\n",
    "image_targets = image_binarizer.fit_transform([r[\"categories\"] for r in rows])\n",
    "image_embeddings = embed_images(paths)\n",
    "\n",
    "print(\n",
    "    \"samples:\",\n",
    "    len(rows),\n",
    "    \"labels:\",\n",
    "    len(image_binarizer.classes_),\n",
    "    \"dims:\",\n",
    "    image_embeddings.shape[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr, te = ensure_label_coverage(image_targets, *split_indices(len(rows)))\n",
    "\n",
    "image_clf = OneVsRestClassifier(LogisticRegression(max_iter=1000, solver=\"liblinear\"))\n",
    "image_clf.fit(image_embeddings[tr], image_targets[tr])\n",
    "\n",
    "image_probs_test = image_clf.predict_proba(image_embeddings[te])\n",
    "image_preds_test = (image_probs_test >= THRESHOLD).astype(int)\n",
    "\n",
    "print(\"micro:\", f1_score(image_targets[te], image_preds_test, average=\"micro\", zero_division=0))\n",
    "print(\"macro:\", f1_score(image_targets[te], image_preds_test, average=\"macro\", zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "### Score and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_probs = image_clf.predict_proba(image_embeddings)\n",
    "image_df = make_df(image_probs, image_binarizer)\n",
    "\n",
    "joblib.dump(\n",
    "    {\"classifier\": image_clf, \"binarizer\": image_binarizer}, ROOT / \"data/models/image.joblib\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_df[top_labels(image_df)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in [\"hockey\", \"browser\", \"food\"]:\n",
    "    plot_hits(top_k(image_df, label, k=8), label=label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "image-activity (.venv)",
   "language": "python",
   "name": "image-activity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
